/*
Generated by EigenEinSum

Recommended filename:
W_g...ScaledEps1Sigma_g,u+v...Rho1_g,r,v...AO1_g,mu,r...AO_g,nu---F1_mu,nu,u.hpp

Einsum expression:
W(g), ScaledEps1Sigma(g, u+v), Rho1(g, r, v), AO1(g, mu, r), AO(g, nu) -> F1(mu, nu, u)

The einsum expression is decomposed into:
W(g), ScaledEps1Sigma(g, u+v) -> WScaledEps1Sigma(g, u+v)
WScaledEps1Sigma(g, u+v), Rho1(g, r, v) -> WScaledEps1SigmaRho1(g, r, u)
WScaledEps1SigmaRho1(g, r, u), AO1(g, mu, r) -> WScaledEps1SigmaRho1AO1(g, mu, u)
WScaledEps1SigmaRho1AO1(g, mu, u), AO(g, nu) -> F1(mu, nu, u)

The index paths are derived to be:
TOP
├── v
│   └── u
│       ├── g
│       └── r
│           └── g
└── u
    ├── r
    │   └── mu
    │       └── g
    └── mu
        └── nu
            └── g
*/
{
	[[maybe_unused]] const int mu_len = AO1.dimension(1);
	assert( mu_len == F1.dimension(0) );
	[[maybe_unused]] const int nu_len = AO.dimension(1);
	assert( nu_len == F1.dimension(1) );
	[[maybe_unused]] const int u_len = F1.dimension(2);
	[[maybe_unused]] const int g_len = W.dimension(0);
	assert( g_len == ScaledEps1Sigma.dimension(0) );
	assert( g_len == Rho1.dimension(0) );
	assert( g_len == AO1.dimension(0) );
	assert( g_len == AO.dimension(0) );
	[[maybe_unused]] const int u_v_len = ScaledEps1Sigma.dimension(1);
	[[maybe_unused]] const int r_len = Rho1.dimension(1);
	assert( r_len == AO1.dimension(2) );
	[[maybe_unused]] const int v_len = Rho1.dimension(2);
	Eigen::Tensor<double, 3> WScaledEps1SigmaRho1(g_len, r_len, u_len);
	Eigen::Tensor<double, 1> WScaledEps1Sigmavu(g_len);
	Eigen::Tensor<double, 2> WScaledEps1SigmaRho1AO1u(g_len, mu_len);
	WScaledEps1SigmaRho1.setZero();
	for ( int v = 0; v < v_len; v++ ){
		for ( int u = 0; u < u_len; u++ ){
			int index_array_u_v[] = {u, v}; std::sort(index_array_u_v, index_array_u_v + 2); [[maybe_unused]] const int u_v = ( index_array_u_v[0] + 0 ) / 1 + ( index_array_u_v[1] + 0 ) * ( index_array_u_v[1] + 1 ) / 2;
			WScaledEps1Sigmavu.setZero();
			double* WScaledEps1Sigmavu_ = &WScaledEps1Sigmavu(0);
			const double* W_ = &W(0);
			const double* ScaledEps1Sigma_u_v = &ScaledEps1Sigma(0, u_v);
			#pragma omp simd simdlen(8) aligned(WScaledEps1Sigmavu_, W_, ScaledEps1Sigma_u_v: 64)
			for ( int g = 0; g < g_len; g++ ){
				WScaledEps1Sigmavu_[g] += W_[g] * ScaledEps1Sigma_u_v[g];
			}
			for ( int r = 0; r < r_len; r++ ){
				double* WScaledEps1SigmaRho1_ru = &WScaledEps1SigmaRho1(0, r, u);
				const double* WScaledEps1Sigmavu_ = &WScaledEps1Sigmavu(0);
				const double* Rho1_rv = &Rho1(0, r, v);
				#pragma omp simd simdlen(8) aligned(WScaledEps1SigmaRho1_ru, WScaledEps1Sigmavu_, Rho1_rv: 64)
				for ( int g = 0; g < g_len; g++ ){
					WScaledEps1SigmaRho1_ru[g] += WScaledEps1Sigmavu_[g] * Rho1_rv[g];
				}
			}
		}
	}
	for ( int u = 0; u < u_len; u++ ){
		WScaledEps1SigmaRho1AO1u.setZero();
		for ( int r = 0; r < r_len; r++ ){
			for ( int mu = 0; mu < mu_len; mu++ ){
				double* WScaledEps1SigmaRho1AO1u_mu = &WScaledEps1SigmaRho1AO1u(0, mu);
				const double* WScaledEps1SigmaRho1_ru = &WScaledEps1SigmaRho1(0, r, u);
				const double* AO1_mur = &AO1(0, mu, r);
				#pragma omp simd simdlen(8) aligned(WScaledEps1SigmaRho1AO1u_mu, WScaledEps1SigmaRho1_ru, AO1_mur: 64)
				for ( int g = 0; g < g_len; g++ ){
					WScaledEps1SigmaRho1AO1u_mu[g] += WScaledEps1SigmaRho1_ru[g] * AO1_mur[g];
				}
			}
		}
		for ( int mu = 0; mu < mu_len; mu++ ){
			for ( int nu = 0; nu < nu_len; nu++ ){
				double _F1munuu = 0;
				const double* WScaledEps1SigmaRho1AO1u_mu = &WScaledEps1SigmaRho1AO1u(0, mu);
				const double* AO_nu = &AO(0, nu);
				#pragma omp simd simdlen(8) reduction(+: _F1munuu) aligned(WScaledEps1SigmaRho1AO1u_mu, AO_nu: 64)
				for ( int g = 0; g < g_len; g++ ){
					_F1munuu += WScaledEps1SigmaRho1AO1u_mu[g] * AO_nu[g];
				}
				F1(mu, nu, u) += _F1munuu;
			}
		}
	}
}
