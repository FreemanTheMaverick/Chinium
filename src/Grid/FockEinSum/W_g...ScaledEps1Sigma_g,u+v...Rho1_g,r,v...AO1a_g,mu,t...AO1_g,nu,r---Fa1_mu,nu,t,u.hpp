/*
Generated by EigenEinSum

Recommended filename:
W_g...ScaledEps1Sigma_g,u+v...Rho1_g,r,v...AO1a_g,mu,t...AO1_g,nu,r---Fa1_mu,nu,t,u.hpp

Einsum expression:
W(g), ScaledEps1Sigma(g, u+v), Rho1(g, r, v), AO1a(g, mu, t), AO1(g, nu, r) -> Fa1(mu, nu, t, u)

The einsum expression is decomposed into:
W(g), ScaledEps1Sigma(g, u+v) -> WScaledEps1Sigma(g, u+v)
WScaledEps1Sigma(g, u+v), Rho1(g, r, v) -> WScaledEps1SigmaRho1(g, r, u)
WScaledEps1SigmaRho1(g, r, u), AO1a(g, mu, t) -> WScaledEps1SigmaRho1AO1a(g, mu, r, t, u)
WScaledEps1SigmaRho1AO1a(g, mu, r, t, u), AO1(g, nu, r) -> Fa1(mu, nu, t, u)

The index paths are derived to be:
TOP
├── v
│   └── u
│       ├── g
│       └── r
│           └── g
└── u
    └── t
        └── r
            └── mu
                ├── g
                └── nu
                    └── g
*/
{
	[[maybe_unused]] const int mu_len = AO1a.dimension(1);
	assert( mu_len == Fa1.dimension(0) );
	[[maybe_unused]] const int nu_len = AO1.dimension(1);
	assert( nu_len == Fa1.dimension(1) );
	[[maybe_unused]] const int t_len = AO1a.dimension(2);
	assert( t_len == Fa1.dimension(2) );
	[[maybe_unused]] const int u_len = Fa1.dimension(3);
	[[maybe_unused]] const int g_len = W.dimension(0);
	assert( g_len == ScaledEps1Sigma.dimension(0) );
	assert( g_len == Rho1.dimension(0) );
	assert( g_len == AO1a.dimension(0) );
	assert( g_len == AO1.dimension(0) );
	[[maybe_unused]] const int u_v_len = ScaledEps1Sigma.dimension(1);
	[[maybe_unused]] const int r_len = Rho1.dimension(1);
	assert( r_len == AO1.dimension(2) );
	[[maybe_unused]] const int v_len = Rho1.dimension(2);
	Eigen::Tensor<double, 3> WScaledEps1SigmaRho1(g_len, r_len, u_len);
	Eigen::Tensor<double, 1> WScaledEps1Sigmavu(g_len);
	Eigen::Tensor<double, 1> WScaledEps1SigmaRho1AO1autrmu(g_len);
	WScaledEps1SigmaRho1.setZero();
	for ( int v = 0; v < v_len; v++ ){
		for ( int u = 0; u < u_len; u++ ){
			int index_array_u_v[] = {u, v}; std::sort(index_array_u_v, index_array_u_v + 2); [[maybe_unused]] const int u_v = ( index_array_u_v[0] + 0 ) / 1 + ( index_array_u_v[1] + 0 ) * ( index_array_u_v[1] + 1 ) / 2;
			WScaledEps1Sigmavu.setZero();
			double* WScaledEps1Sigmavu_ = &WScaledEps1Sigmavu(0);
			const double* W_ = &W(0);
			const double* ScaledEps1Sigma_u_v = &ScaledEps1Sigma(0, u_v);
			#pragma omp simd simdlen(8) aligned(WScaledEps1Sigmavu_, W_, ScaledEps1Sigma_u_v: 64)
			for ( int g = 0; g < g_len; g++ ){
				WScaledEps1Sigmavu_[g] += W_[g] * ScaledEps1Sigma_u_v[g];
			}
			for ( int r = 0; r < r_len; r++ ){
				double* WScaledEps1SigmaRho1_ru = &WScaledEps1SigmaRho1(0, r, u);
				const double* WScaledEps1Sigmavu_ = &WScaledEps1Sigmavu(0);
				const double* Rho1_rv = &Rho1(0, r, v);
				#pragma omp simd simdlen(8) aligned(WScaledEps1SigmaRho1_ru, WScaledEps1Sigmavu_, Rho1_rv: 64)
				for ( int g = 0; g < g_len; g++ ){
					WScaledEps1SigmaRho1_ru[g] += WScaledEps1Sigmavu_[g] * Rho1_rv[g];
				}
			}
		}
	}
	for ( int u = 0; u < u_len; u++ ){
		for ( int t = 0; t < t_len; t++ ){
			for ( int r = 0; r < r_len; r++ ){
				for ( int mu = 0; mu < mu_len; mu++ ){
					WScaledEps1SigmaRho1AO1autrmu.setZero();
					double* WScaledEps1SigmaRho1AO1autrmu_ = &WScaledEps1SigmaRho1AO1autrmu(0);
					const double* WScaledEps1SigmaRho1_ru = &WScaledEps1SigmaRho1(0, r, u);
					const double* AO1a_mut = &AO1a(0, mu, t);
					#pragma omp simd simdlen(8) aligned(WScaledEps1SigmaRho1AO1autrmu_, WScaledEps1SigmaRho1_ru, AO1a_mut: 64)
					for ( int g = 0; g < g_len; g++ ){
						WScaledEps1SigmaRho1AO1autrmu_[g] += WScaledEps1SigmaRho1_ru[g] * AO1a_mut[g];
					}
					for ( int nu = 0; nu < nu_len; nu++ ){
						double _Fa1munutu = 0;
						const double* WScaledEps1SigmaRho1AO1autrmu_ = &WScaledEps1SigmaRho1AO1autrmu(0);
						const double* AO1_nur = &AO1(0, nu, r);
						#pragma omp simd simdlen(8) reduction(+: _Fa1munutu) aligned(WScaledEps1SigmaRho1AO1autrmu_, AO1_nur: 64)
						for ( int g = 0; g < g_len; g++ ){
							_Fa1munutu += WScaledEps1SigmaRho1AO1autrmu_[g] * AO1_nur[g];
						}
						Fa1(mu, nu, t, u) += _Fa1munutu;
					}
				}
			}
		}
	}
}
